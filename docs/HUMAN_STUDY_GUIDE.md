# Human Study Guide

This document provides instructions for conducting human evaluation studies to measure the legibility of generated solutions.

## Overview

The human study follows the methodology described in the paper, where time-constrained humans evaluate the correctness of generated solutions. This helps measure the legibility of the prover's outputs to human evaluators.

## Study Design

### 1. Participants

- **Number**: 75 participants (as in the paper)
- **Requirements**: Basic math knowledge, no advanced mathematical training required
- **Compensation**: Based on completion, not correctness
- **Time Limit**: 45 seconds per problem (as in the paper)

### 2. Task Description

Participants are asked to:
1. Read a math problem and its proposed solution
2. Determine if the solution is correct or incorrect
3. Respond within the 45-second time limit
4. Make their best judgment even if time runs out

### 3. Data Collection

- **Problems**: GSM8K test set problems
- **Solutions**: Generated by different prover models/rounds
- **Balance**: Equal number of correct and incorrect solutions
- **Randomization**: Random order of presentation
- **Exclusion**: Remove participants with chance-level performance

## Implementation

### 1. Human Proxy Evaluation

For automated evaluation, use the human proxy evaluator:

```python
from pvg_core.eval.human_proxy import HumanProxyEvaluator

# Initialize evaluator
evaluator = HumanProxyEvaluator(
    judge_model=small_lm_model,
    tokenizer=tokenizer,
    time_limit=45.0,
    device="cuda"
)

# Evaluate solutions
results = evaluator.evaluate_batch(
    problems=problems,
    solutions=solutions,
    ground_truths=ground_truths,
    evaluator_id="proxy"
)

# Calculate metrics
metrics = evaluator.calculate_human_metrics(results)
```

### 2. Real Human Evaluation

For actual human studies, implement a web interface:

```python
# Example Flask app for human evaluation
from flask import Flask, render_template, request, jsonify
import time
import json

app = Flask(__name__)

@app.route('/evaluate', methods=['POST'])
def evaluate_solution():
    data = request.json
    problem = data['problem']
    solution = data['solution']
    start_time = time.time()
    
    # Present to human evaluator
    # ... (implement UI)
    
    # Collect response
    response = {
        'problem': problem,
        'solution': solution,
        'human_judgment': data['judgment'],  # True/False
        'response_time': time.time() - start_time,
        'confidence': data['confidence'],  # 1-5 scale
        'evaluator_id': data['evaluator_id'],
        'timestamp': time.time()
    }
    
    return jsonify(response)
```

## Metrics

### 1. Primary Metrics

- **Accuracy**: Percentage of correct judgments
- **Response Time**: Average time per evaluation
- **Confidence**: Self-reported confidence level
- **Timeout Rate**: Percentage of evaluations that hit time limit

### 2. Legibility Metrics

- **Helpful Accuracy**: Accuracy on helpful prover solutions
- **Sneaky Accuracy**: Accuracy on sneaky prover solutions
- **Legibility Improvement**: Change in accuracy over training rounds

### 3. Statistical Analysis

```python
import scipy.stats as stats
import numpy as np

def analyze_legibility_improvement(round_results):
    """Analyze legibility improvement across rounds."""
    
    # Extract accuracy by round
    accuracies = []
    for round_num, results in round_results.items():
        metrics = evaluator.calculate_human_metrics(results)
        accuracies.append(metrics['accuracy'])
    
    # Statistical test
    if len(accuracies) > 1:
        # Paired t-test
        t_stat, p_value = stats.ttest_rel(accuracies[1:], accuracies[:-1])
        
        # Effect size
        effect_size = np.mean(accuracies[1:]) - np.mean(accuracies[:-1])
        
        return {
            't_statistic': t_stat,
            'p_value': p_value,
            'effect_size': effect_size,
            'significant': p_value < 0.05
        }
    
    return None
```

## Study Protocol

### 1. Pre-Study Setup

```bash
# Prepare evaluation data
python scripts/prepare_human_eval.py \
  --model-dir outputs/round_001 \
  --data-dir data/gsm8k \
  --output-dir outputs/human_eval \
  --num-samples 100 \
  --balance-ratio 0.5
```

### 2. Data Collection

```bash
# Run human evaluation
python scripts/run_human_eval.py \
  --eval-data outputs/human_eval/eval_data.json \
  --output-dir outputs/human_results \
  --time-limit 45 \
  --max-participants 75
```

### 3. Analysis

```bash
# Analyze results
python scripts/analyze_human_eval.py \
  --results-dir outputs/human_results \
  --output-dir outputs/analysis \
  --statistical-tests
```

## Expected Results

### 1. Baseline Performance

- **Random Performance**: 50% accuracy
- **Human Performance**: 70-80% accuracy (time-constrained)
- **Full Time Performance**: 85-90% accuracy

### 2. Legibility Improvement

- **Round 0**: Baseline legibility
- **Round 1-3**: Gradual improvement
- **Round 4+**: Plateau or continued improvement

### 3. Role Differences

- **Helpful Solutions**: Higher accuracy, faster response
- **Sneaky Solutions**: Lower accuracy, slower response
- **Convergence**: Solutions become more distinguishable

## Quality Control

### 1. Participant Screening

```python
def screen_participants(results, chance_threshold=0.6):
    """Remove participants with chance-level performance."""
    
    participant_scores = {}
    for result in results:
        evaluator_id = result['evaluator_id']
        if evaluator_id not in participant_scores:
            participant_scores[evaluator_id] = []
        
        participant_scores[evaluator_id].append(result['human_judgment'])
    
    # Calculate accuracy per participant
    participant_accuracies = {}
    for evaluator_id, judgments in participant_scores.items():
        accuracy = sum(judgments) / len(judgments)
        participant_accuracies[evaluator_id] = accuracy
    
    # Filter out chance-level performers
    valid_participants = [
        evaluator_id for evaluator_id, accuracy in participant_accuracies.items()
        if accuracy > chance_threshold
    ]
    
    return valid_participants
```

### 2. Data Validation

```python
def validate_human_data(results):
    """Validate human evaluation data."""
    
    issues = []
    
    for result in results:
        # Check response time
        if result['response_time'] > 60:  # Over time limit
            issues.append(f"Response time too long: {result['response_time']}")
        
        # Check confidence range
        if not (1 <= result['confidence'] <= 5):
            issues.append(f"Invalid confidence: {result['confidence']}")
        
        # Check judgment type
        if not isinstance(result['human_judgment'], bool):
            issues.append(f"Invalid judgment type: {result['human_judgment']}")
    
    return issues
```

## Reporting

### 1. Generate Report

```bash
# Generate comprehensive report
python scripts/generate_human_report.py \
  --results-dir outputs/human_results \
  --output-dir outputs/reports \
  --include-plots \
  --include-statistics
```

### 2. Key Findings

- **Legibility Improvement**: Quantify improvement over rounds
- **Role Separation**: Measure helpful vs sneaky distinction
- **Time Efficiency**: Analyze response time trends
- **Confidence Calibration**: Assess confidence accuracy

### 3. Visualizations

- **Accuracy by Round**: Line plot showing improvement
- **Response Time Distribution**: Histogram of response times
- **Confidence vs Accuracy**: Scatter plot
- **Role Comparison**: Bar chart of helpful vs sneaky performance

## Ethical Considerations

### 1. Participant Consent

- Clear explanation of the task
- Time commitment disclosure
- Data usage agreement
- Right to withdraw

### 2. Data Privacy

- Anonymize participant data
- Secure data storage
- Limited data retention
- No personal information collection

### 3. Fair Compensation

- Reasonable payment for time
- No performance-based compensation
- Clear payment terms
- Timely payment processing

## Troubleshooting

### 1. Common Issues

**Low Participation**:
- Increase compensation
- Simplify instructions
- Reduce time commitment
- Improve user interface

**Poor Data Quality**:
- Improve screening criteria
- Add training examples
- Provide feedback
- Monitor performance

**Technical Issues**:
- Test interface thoroughly
- Provide technical support
- Backup data collection
- Monitor system performance

### 2. Quality Assurance

- Pilot study with small group
- Test all functionality
- Validate data collection
- Monitor participant feedback

## References

- Original paper: Kirchner et al. (2024)
- Human evaluation methodology: Section 4.1
- Statistical analysis: Section 4.2
- Results interpretation: Section 4.3
